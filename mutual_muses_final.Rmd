---
title: "Mutual Muses Final Report"
author: "Matthew Lincoln"
date: "November 1, 2017"
output: 
  ioslides_presentation:
    smaller: true
    transition: faster
    fig_height: 4.5
    standalone: true
---

```{r libraries setup, include=FALSE}
library(tidyverse)
library(stringr)
library(tidytext)
library(textreuse)
library(jsonlite)
library(lubridate)
library(knitr)
library(uaparserjs)
library(DT)
library(forcats)
library(purrrlyr)
library(ggplot2)
library(ggrepel)
library(bootr)
library(rematch2)

opts_chunk$set(cache = TRUE, echo = FALSE, warning = FALSE, message = FALSE)
```

```{r processing_functions}
# Need to try two different field names to collect every single file name
pull_filename <- function(x, .default = NA_character_) {
  if (!is.null(x[["Filename"]])) {
    x[["Filename"]]
  } else if (!is.null(x[["file_name"]])) {
    x[["file_name"]]
  } else {
    .default
  }
}

# General function applicable to all workflows. Takes raw Zooniverse csv and
# returns additional parses along with the metadata, annotations, and subjects
# data as R lists.
parse_zooniverse_metadata <- function(raw) {
  
  message("Reading metadata...")
  meta <- map(raw$metadata, fromJSON)
  message("Reading annotations...")
  annotations <- map(raw$annotations, fromJSON)
  message("Reading subjects...")
  subjects <- map(raw$subject_data, fromJSON)
  
  message("Computing useful variables...")
  records <- raw %>% 
    # Get necessary user agent 
    select(
      classification_id,
      workflow_version,
      subject_ids,
      user_name,
      created_at) %>% 
    # Get relevant classification metadata
    mutate(
      started_on = map_chr(meta, "started_at", .default = NA_character_),
      ended_on = map_chr(meta, "finished_at", .default = NA_character_),
      user_agent = map_chr(meta, "user_agent", .default = NA_character_),
      file_name = map(subjects, 1) %>% map_chr(pull_filename, .default = NA_character_),
      subject_loaded_date = map_chr(subjects, list(1, "retired", "created_at"), .default = NA_character_),
      subject_retired_date = map_chr(subjects, list(1, "retired", "retired_at"), .default = NA_character_)) %>% 
    # Coerce variables to useful types
    mutate_at(vars(created_at, started_on, ended_on, subject_loaded_date, subject_retired_date), funs(ymd_hms)) %>% 
    # Generate further useful metrics based on thse metadata
    mutate(
      is_retired = !is.na(subject_retired_date),
      day_of = floor_date(created_at, unit = "day"),
      is_weekend = wday(created_at) %in% c(1, 2),
      seconds_annotating = as.numeric(ended_on - started_on),
      minutes_annotating = seconds_annotating / 60) %>% 
    add_count(file_name) %>% 
    rename(n_annotations = n) %>% 
      # Parse the user agent data for each annotation
    bind_cols(ua_parse(.$user_agent))
    
    list(records = records, annotations = annotations, meta = meta, subjects = subjects)
}

workflow_filter <- function(zparse, workflow_versions) {
  
  indices <- which(zparse$records$workflow_version %in% workflow_versions)
  records <- slice(zparse$records, indices)
  annotations <- zparse$annotations[indices]
  meta <- zparse$meta[indices]
  subjects <- zparse$subjects[indices]
  
  list(records = records, annotations = annotations, meta = meta, subjects = subjects)
}

parse_transcription_annotations <- function(zparse, workflow_versions) {
  
  filtered_zparse <- workflow_filter(zparse, workflow_versions)
  
  filtered_zparse$records %>% 
    mutate(
      annotation_text = map_chr(filtered_zparse$annotations, list("value", 1), .default = NA_character_),
      contains_special = map_chr(filtered_zparse$annotations, list("value", 2, 1), .default = NA_character_) == "Yes",
      unclear_count = str_count(annotation_text, "\\[unclear\\]"),
      clean_transcription = str_replace_all(annotation_text, "\\[/?unclear\\]", "") %>% iconv(to = "ASCII//TRANSLIT"),
      charcount = nchar(clean_transcription)
    )
}

parse_image_annotations <- function(zparse, workflow_versions) {
  
  filtered_zparse <- workflow_filter(zparse, workflow_versions)

  filtered_zparse$records %>% 
    mutate(
      drawing_answer = map(filtered_zparse$annotations, "value") %>% map_chr(paste, collapse = ""),
      has_drawing = case_when(
        drawing_answer == "No, there isn't." ~ FALSE,
        drawing_answer == "Yes, there is!" ~ TRUE,
        drawing_answer == "No." ~ FALSE,
        drawing_answer == "Yes." ~ TRUE,
        TRUE ~ FALSE
      )
    )
}
      
dissimilar_heatmap <- function(doc) {
  simil <- doc %>% 
    pairwise_compare(jaccard_dissimilarity) %>% 
    as.data.frame() 
  
  factored_simil <- simil %>% 
    rownames_to_column("source") %>% 
    gather(target, value = dissimilarity, -source) %>% 
    mutate_at(vars(target, source), funs(factor(., levels = rownames(simil)))) %>% 
    filter(!is.na(dissimilarity))
  
  ggplot(factored_simil, aes(x = source, y = target, fill = dissimilarity)) +
    scale_fill_distiller(palette = "YlOrRd", direction = 1, limits = c(0, 1)) +
    geom_raster() +
    labs(x = NULL, y = NULL) +
    theme(
      # Rotate the x-axis lables so they are legible
      axis.text.x = element_text(angle = 270, hjust = 0),
      aspect.ratio = 1)
}

side_by_side <- function(subj, left, right) {
  datatable(data.frame(
    a = as.character(content(subj[[left]])),
    b = as.character(content(subj[[right]]))) %>% 
      set_names(c(left, right)), escape = TRUE)
}

# Ratio of items in b that are in a
doc_ratio <- function(doc) {
  doc %>% 
    as.data.frame() %>% 
    rownames_to_column("a") %>% 
    gather(b, value, -a) %>% 
    filter(!is.na(value))
}

ratio_plot <- function(ratio_data) {
  ggplot(ratio_data, aes(x = a, y = b, fill = value)) +
  geom_raster() +
  scale_fill_distiller(palette = "YlGnBu", direction = 1, limits = c(0, 1)) +
  theme(
    axis.text.x = element_text(angle = 270, hjust = 0),
    aspect.ratio = 1) +
  labs(y = "Ratio of transcription...", x = "...contained by transcription:")
}

summarize_ratios <- function(doc) {
  doc %>% 
    doc_ratio() %>% 
    group_by(a) %>% 
    summarize(
      mean_ratio = mean(value),
      median_ratio = median(value),
      max_ratio = max(value),
      min_ratio = min(value),
      total_ratio = sum(value))
}

```

```{r read_zooniverse_files}
raw_export <- read_csv("data/mutual-muses-classifications.csv", col_types = "icicicccccccci")

count(raw_export, workflow_version, sort = TRUE)

# Do a preliminary parse of all data
intermediate_process <- parse_zooniverse_metadata(raw_export)

image_workflows <- c("5.17")

transcription_workflows <- c("28.76", "29.76")

drawing_classifications <- parse_image_annotations(intermediate_process, workflow_versions = image_workflows) %>% 
  # Get rid of one weird classification that contained transcription answers
  # despite having the workflow of a drawing classification
  filter(classification_id != 73150258)

transcription_classifications <- parse_transcription_annotations(intermediate_process, workflow_versions = transcription_workflows)

intermediate_process$records %>% 
  filter(created_at > ymd(20170630)) %>% 
  count(workflow_version)
  
  ggplot(aes(x = created_at, fill = workflow_version)) + 
  geom_histogram()

```

## Distribution of work per transcriber

```{r top_transcribers}
transcription_classifications %>% 
  count(user_name) %>% 
  ggplot(aes(x = n)) +
  geom_histogram(binwidth = 1) +
  labs(x = "No. of transcriptions", y = "No. of individual users") +
  scale_x_sqrt() +
  scale_y_sqrt()
```

## Power transcribers

```{r transcribers_by_name}
transcription_classifications %>% 
  count(user_name, sort = TRUE) %>% 
  datatable()
```


## How did engagement change over time?

Weekends are higlighted in blue.

```{r transcription_times}
release_annotations <- data_frame(
  year = c("1948", "1949", "1950", "1951", "1952", "Getty Hub 1", "Getty Hub 2", "Getty Hub 3", "Newsletter"),
  release_date = ymd_hms(20170801000000, 20170802000000, 20170804000000, 20170814000000, 20170829000000, 20170802000000, 20170804000000, 20170816000000, 20170831000000)
)

class_per_day <- comb_df %>% 
  filter(day_of > ymd_hms(20170730000000)) %>% 
  add_column(counter = 1) %>% 
  bootr(group_vars = c("day_of", "is_weekend"), source_var = "counter", boot_fun = sum)

class_per_day %>% 
  ggplot(aes(x = day_of, y = boot_med)) +
  #geom_line() +
  geom_errorbar(aes(ymin = boot_low, ymax = boot_high), alpha = 0.5) +
  geom_point(aes(color = is_weekend), size = 3) +
  theme(legend.position = "none") +
  labs(x = NULL, y = "No. of transcriptions") +
  geom_vline(data = release_annotations, aes(xintercept = release_date), alpha = 0.5) +
  geom_label_repel(data = release_annotations, aes(x = release_date, y = 1000, label = year), min.segment.length = unit(0, "lines"))
```

## Transcription lengths

```{r}
trans_medians <- comb_df %>% 
  group_by(phase) %>% 
  summarize(median_length = median(charcount))

comb_df %>% 
  ggplot() +
  geom_histogram(aes(x = charcount, fill = phase), alpha = 0.5) +
  geom_vline(data = trans_medians, aes(xintercept = median_length, color = phase)) +
  facet_wrap(~phase, scales = "free_y", ncol = 1) +
  labs(x = "No. of characters in the transcription", y = "No. of transcriptions")
```


## Transcription time

```{r time_spent_transcribing}
med_times <- comb_df %>% 
  group_by(phase) %>% 
  summarize(med_min = median(minutes_transcribing))

ggplot(comb_df, aes(x = minutes_transcribing, fill = phase)) +
  geom_histogram(alpha = 0.4) +
  geom_vline(data = med_times, aes(xintercept = med_min, color = phase)) +
  xlim(0, 30) +
  facet_wrap(~ phase, scales = "free_y", ncol = 1) +
  labs(x = "Minutes transcribing", y = "No. of transcriptions")

```


## Use of `[unclear]` tags.

```{r unclear_counts}
ratio_with_unclear <- mean(class_df()$unclear_count > 0)

ggplot(class_df(), aes(x = unclear_count)) +
  geom_histogram() +
  labs(x = "Times '[unclear]' used", y = "No. of transcriptions")
```
 
About `r scales::percent(ratio_with_unclear)` of transcriptions used the `[unclear]` tag at least once.
 
## Use of `[unclear]` tags.

```{r unclear_text_examples}
unclear_words <- re_match_all(class_df()$annotation_text, "\\[unclear\\](?<unclear_text>.*?)\\[/unclear\\]") %>% 
  unnest()

sample(unclear_words$unclear_text, size = 70, replace = FALSE)
```

## Special characters

```{r non_ascii}
ratio_special_chars <- mean(class_df()$contains_special)
```

`r scales::percent(ratio_special_chars)` of transcriptions were marked as "Contains special characters"

```{r non_ascii_examples}
diacritic_chars <- "[àèìòùÀÈÌÒÙáéíóúýÁÉÍÓÚÝâêîôûÂÊÎÔÛãñõÃÑÕäëïöüÿÄËÏÖÜŸçÇßØøÅåÆæœ]"

class_df() %>% 
  bind_re_match(annotation_text, paste0("(?<spec_char_context>.{0,20}", diacritic_chars, "+.{0,20})")) %>% 
  select(spec_char_context) %>% 
  na.omit() %>% 
  datatable()
```

## Judging Transcription Consensus

Even when excluding blank transcriptions, there a handful of transcriptions with too few characters to properly tokenize. Since these transcriptions must be discarded, we have a few documents that only have 1/2 useful transcriptions.

```{r create_corpora}
# Produce a list of corpus objects
trans_corpus <- class_df() %>% 
  group_by(file_name) %>% 
  split(.$file_name) %>% 
  map(function(x) {
    TextReuseCorpus(text = set_names(x$clean_transcription, x$classification_id), tokenizer = tokenizers::tokenize_character_shingles, simplify = TRUE, n = 4, keep_tokens = TRUE, keep_text = TRUE, progress = FALSE, skip_short = FALSE)
  })

trans_corpus %>% 
  map_int(length) %>% 
  as.factor() %>% 
  fct_count() %>% 
  rename(elligible_transcriptions = f)
```

## Pairwise-similarity summaries

Compared to the beta results, transcriptions here tend to have more consensus.

High-disagreement docs are ones with too few transcriptions. Suggest re-loading some docs.

```{r beta_similarity}

```

```{r all_pairwise_similarity}
all_dissimilarity <- trans_corpus %>% 
  map(pairwise_compare, f = jaccard_dissimilarity, progress = FALSE) %>% 
  map_df(function(x) {
    data_frame(
      mean_dissimilarity = mean(x, na.rm = TRUE),
      median_dissimilarity = median(x, na.rm = TRUE),
      max_dissimilarity = max(x, na.rm = TRUE),
      min_dissimilarity = min(x, na.rm = TRUE))
  }, .id = "subject_id")
```

```{r overall_dissimilarity}
all_dissimilarity %>% 
  gather(metric, value, -subject_id) %>% 
  ggplot(aes(x = metric, y = value, fill = metric)) +
  geom_boxplot() +
  ylim(0, 1) +
  theme(legend.position = "none")

low_example <- "gri_2003_m_46_b02_f01_002.jpg"
low_dissimilarity_doc <- trans_corpus[[low_example]]
high_example <- "gri_2003_m_46_b01_f01_002.jpg"
high_dissimilarity_doc <- trans_corpus[["gri_2003_m_46_b01_f01_002.jpg"]]
```


```{r pairwise_ratios}
ratio_contained <- trans_corpus %>% 
  map(pairwise_compare, f = ratio_of_matches, directional = TRUE, progress = FALSE)
```

```{r find_winning_transcriptions}
transcription_ratios <- ratio_contained %>% 
  map_df(summarize_ratios, .id = "file_name") %>% 
  mutate(
    file_name = as.character(file_name),
    classification_id = as.integer(a)) %>% 
  select(-a)

winning_df <- class_df() %>% 
  left_join(transcription_ratios, by = c("file_name", "classification_id")) %>% 
  left_join(all_dissimilarity, by = c("file_name" = "subject_id")) %>% 
  group_by(file_name) %>% 
  mutate(
    selected_as_winning = row_number(desc(median_ratio)) == 1,
    is_winning = ifelse(selected_as_winning, "winner", "loser")) %>%
  ungroup() %>% 
  arrange(file_name)

saveRDS(winning_df, "data/derived/all_with_winning.rds")
```
